{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites\n",
        "\n",
        "To get the most out of this content, it is important to be comfortable with Python programming, have a basic understanding of deep learning concepts and transformers, and be familiar with the Pytorch framework. The complete source code will be available on GitHub.\n",
        "\n",
        "Before diving into the core implementation, we need to install and import the relevant libraries. Also, it is important to note that the training script is inspired by this repository from Hugging Face."
      ],
      "metadata": {
        "id": "pym1z2VouttQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library installation\n",
        "The installation process is detailed below:\n",
        "\n",
        "First of all, we use the %%bash statement to run the install commands in a single cell as a bash command in the Jupyter Notebook.\n",
        "\n",
        "Trl: used to train transformer language models with reinforcement learning.\n",
        "Peft uses the parameter-efficient fine-tuning (PEFT) methods to enable efficient adaptation of the pre-trained model.\n",
        "Torch: a widely-used open-source machine learning library.\n",
        "Datasets: used to assist in downloading and loading many common machine learning datasets.\n",
        "Transformers: a library developed by Hugging Face and comes with thousands of pre-trained models for a variety of text-based tasks such as classification, summarization, and translation."
      ],
      "metadata": {
        "id": "pXzOctsquwyQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIEcbBcMiUb9",
        "outputId": "46bd0b54-29d0-46d9-cff7-58cd16067757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 310.9/310.9 kB 4.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 480.6/480.6 kB 12.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 5.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.3/179.3 kB 7.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 5.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 8.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip -q install trl\n",
        "pip -q install peft\n",
        "pip -q install torch\n",
        "pip -q install datasets\n",
        "pip -q install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "QscF0DQLvBDX",
        "outputId": "6b54bf58-4452-49f9-8338-912e67bb6fc3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'prepare_model_for_int8_training' from 'peft' (/usr/local/lib/python3.10/dist-packages/peft/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-206b28e447d5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_model_for_int8_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'prepare_model_for_int8_training' from 'peft' (/usr/local/lib/python3.10/dist-packages/peft/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preparation\n",
        "The alpaca dataset, freely available on Hugging Face, will be used for this illustration. The dataset has three main columns: instructions, input, and output. These columns are combined to generate a final text column.\n",
        "\n",
        "The instruction to load the dataset is given below by providing the name of the dataset of interest, which is tatsu-lab/alpaca:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y7w1ruS0vFQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "id": "wEM9BZ_6vGjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the resulting data is in a dictionary of two keys:\n",
        "\n",
        "- Features: containing the main columns of the data\n",
        "- Num_rows: corresponding to the total number of rows in the data\n",
        "\n",
        "The first five rows can be displayed with the following instruction. First, convert the dictionary into a pandas dataframe, then display the rows."
      ],
      "metadata": {
        "id": "R_2Ud-wKvJ_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_format = train_dataset.to_pandas()\n",
        "display(pandas_format.head())"
      ],
      "metadata": {
        "id": "VQ0lWA0jvRlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For better visualization, let’s print the information about the first three rows, but before that, we need to install the textwrap library to set the maximum number of words per line to 50. The first print statement separates each block by 15 dashes."
      ],
      "metadata": {
        "id": "dUTfd8wJvXYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n"
      ],
      "metadata": {
        "id": "Th62NjczvZrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(3):\n",
        "   print(\"---\"*15)\n",
        "   print(\"Instruction:\n",
        "       {}\".format(textwrap.fill(pandas_format.iloc[index][\"instruction\"],\n",
        "       width=50)))\n",
        "   print(\"Output:\n",
        "       {}\".format(textwrap.fill(pandas_format.iloc[index][\"output\"],\n",
        "       width=50)))\n",
        "   print(\"Text:\n",
        "       {}\".format(textwrap.fill(pandas_format.iloc[index][\"text\"],\n",
        "       width=50)))"
      ],
      "metadata": {
        "id": "3obpIDR9vay6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "Before proceeding to train the model, we need to set up some prerequisites:\n",
        "\n",
        "- Pre-trained Model: We will use the pre-trained model Salesforce/xgen-7b-8k-base, which is available on Hugging Face. Salesforce trained this series of 7B LLMs named XGen-7B with standard dense attention on up to 8K sequences for up to 1.5T tokens.\n",
        "- Tokenizer: This is needed for tokenization tasks on the training data. The code to load the pre-trained model and tokenizer is as follows:"
      ],
      "metadata": {
        "id": "ZIUdaW4bvfnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_name = \"Salesforce/xgen-7b-8k-base\"\n",
        "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "z9dN4noavm87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training configuration\n",
        "The training requires some training arguments and configurations, and the two important configuration objects are defined below, an instance of the TrainingArguments, an instance of the LoraConfig model, and finally the SFTTrainer model.\n",
        "\n",
        "### TrainingArguments\n",
        "This is used to define the parameters for model training.\n",
        "\n",
        "In this specific scenario, we start by defining the destination where the trained model will be stored using the output_dir attribute before defining additional hyperparameters, such as the optimization method, the learning rate, the number of epochs, and more."
      ],
      "metadata": {
        "id": "kskdbgRQvpLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_training_args = TrainingArguments(\n",
        "       output_dir=\"xgen-7b-8k-base-fine-tuned\",\n",
        "       per_device_train_batch_size=4,\n",
        "       optim=\"adamw_torch\",\n",
        "       logging_steps=80,\n",
        "       learning_rate=2e-4,\n",
        "       warmup_ratio=0.1,\n",
        "       lr_scheduler_type=\"linear\",\n",
        "       num_train_epochs=1,\n",
        "       save_strategy=\"epoch\"\n",
        "   )"
      ],
      "metadata": {
        "id": "zR0SbZCXvuEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRAConfig\n",
        "The main arguments used for this scenario are the rank of the low-rank transformation matrix in LoRA, which is set to 16. Then, the scaling factor for the additional parameters in LoRA is set to 32.\n",
        "\n",
        "Furthermore, the dropout ratio is 0.05, meaning that 5% of the input units will be ignored during the training. Finally, since we are dealing with a causual language modeling, the task is hence initialized with the CAUSAL_LM attribute."
      ],
      "metadata": {
        "id": "7xQpShvhvw91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SFTTrainer\n",
        "This aims to train the model using the training data, the tokenizer, and additional information such as the above models.\n",
        "\n",
        "Since we are using the text field from the training data, it is important to have a look at the distribution in order to help in setting the maximum number of tokens in a given sequence."
      ],
      "metadata": {
        "id": "7SkfH0Jiv2VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "pandas_format['text_length'] = pandas_format['text'].apply(len)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(pandas_format['text_length'], bins=50, alpha=0.5, color='g')\n",
        "plt.title('Distribution of Length of Text')\n",
        "plt.xlabel('Length of Text')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hodp6-eMvx70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above observation, we can see that the majority of text has a length between 0 and 1000. Also, we can see below that only 4.5% percent of the text documents have a length greater than 1024."
      ],
      "metadata": {
        "id": "HyiyV7pnv6ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = pandas_format['text_length'] > 1024\n",
        "percentage = (mask.sum() / pandas_format['text_length'].count()) * 100\n",
        "\n",
        "\n",
        "print(f\"The percentage of text documents with a length greater than 1024 is: {percentage}%\")\n"
      ],
      "metadata": {
        "id": "u_GeT2Xvv8F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we set the maximum number of tokens in the sequence to 1024 so that any text longer than this get truncated."
      ],
      "metadata": {
        "id": "sK7tqLHcv9e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SFT_trainer = SFTTrainer(\n",
        "       model=model,\n",
        "       train_dataset=train_dataset,\n",
        "       dataset_text_field=\"text\",\n",
        "       max_seq_length=1024,\n",
        "       tokenizer=tokenizer,\n",
        "       args=model_training_args,\n",
        "       packing=True,\n",
        "       peft_config=lora_peft_config,\n",
        "   )"
      ],
      "metadata": {
        "id": "uycViJThv-j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training execution\n",
        "With all the prerequisites in place, we can now run the training process of the model as follows:"
      ],
      "metadata": {
        "id": "wqEhXjHBwBEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = prepare_model_for_int8_training(model)\n",
        "model = get_peft_model(model, lora_peft_config)\n",
        "training_args = model_training_args\n",
        "trainer = SFT_trainer\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1T9UvCyEwFKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to mention that this training is being performed on a cloud environment with a GPU, which makes the overall training process faster. However, training on a local computer would require more time to complete.\n",
        "\n",
        "Our blog, Pros and Cons of Using LLMs in the Cloud Versus Running LLMs Locally, provides key considerations for selecting the optimal deployment strategy for LLMs\n",
        "\n",
        "Let’s understand what is happening in the above code snippet:\n",
        "\n",
        "- tokenizer.pad_token = tokenizer.eos_token: Sets padding token to be the same as the end-of-sentence token.\n",
        "- model.resize_token_embeddings(len(tokenizer)): Resizes the token embedding layer of the model to match the length of the tokenizer vocabulary.\n",
        "- model = prepare_model_for_int8_training(model): Prepares the model for training with INT8 precision, likely performing quantization.\n",
        "- model = get_peft_model(model, lora_peft_config): Adjusts the given model according to the PEFT configuration.\n",
        "- training_args = model_training_args: Assigns predefined training arguments to training_args.\n",
        "- trainer = SFT_trainer: Assigns the SFTTrainer instance to the variable trainer.\n",
        "- trainer.train(): Triggers the training process of the model according to the provided specifications."
      ],
      "metadata": {
        "id": "d_be5bsywEH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "This article has provided a clear guide on training a large language model using PyTorch. Starting with the dataset preparation, it walked through the steps of preparing the prerequisites, setting up the trainer, and finally, running the training process.\n",
        "\n",
        "Although it used a specific dataset and pre-trained model, the process should be largely the same for any other compatible options. Now that you understand how to train an LLM, you can leverage this knowledge to train other sophisticated models for various NLP tasks."
      ],
      "metadata": {
        "id": "60np5iH6wVGk"
      }
    }
  ]
}